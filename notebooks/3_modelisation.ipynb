{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c28e522",
   "metadata": {},
   "source": [
    "# Modélisation - Challenge Titanic\n",
    "\n",
    "Ce notebook vous guide à travers le processus de modélisation pour prédire la survie des passagers du Titanic. Nous allons tester différents algorithmes de classification et optimiser leurs performances.\n",
    "\n",
    "## Objectifs de la modélisation:\n",
    "- Tester plusieurs algorithmes de classification\n",
    "- Évaluer les performances des modèles avec validation croisée\n",
    "- Optimiser les hyperparamètres des modèles les plus prometteurs\n",
    "- Sélectionner le meilleur modèle pour les prédictions finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c17fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour la modélisation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# Algorithmes de classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Pour charger les données prétraitées\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Pour afficher les graphiques dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Pour afficher toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f6a56",
   "metadata": {},
   "source": [
    "## 1. Chargement des données prétraitées\n",
    "\n",
    "Commençons par charger les données prétraitées lors de l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers les données prétraitées\n",
    "preprocessed_dir = '/workspaces/titanicML/Data/preprocessed'\n",
    "preprocessed_file = os.path.join(preprocessed_dir, 'preprocessed_data.pkl')\n",
    "\n",
    "try:\n",
    "    # Charger les données prétraitées\n",
    "    preprocessed_data = joblib.load(preprocessed_file)\n",
    "    \n",
    "    # Extraire les éléments du dictionnaire\n",
    "    X_train = preprocessed_data['train_features']\n",
    "    y_train = preprocessed_data['train_target']\n",
    "    X_test = preprocessed_data['test_features']\n",
    "    test_passenger_ids = preprocessed_data['test_passenger_ids']\n",
    "    feature_names = preprocessed_data['feature_names']\n",
    "    scaler = preprocessed_data['scaler']\n",
    "    \n",
    "    print(\"Données prétraitées chargées avec succès!\")\n",
    "    print(f\"Ensemble d'entraînement: {X_train.shape[0]} observations, {X_train.shape[1]} caractéristiques\")\n",
    "    print(f\"Ensemble de test: {X_test.shape[0]} observations, {X_test.shape[1]} caractéristiques\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur: Fichier {preprocessed_file} introuvable.\")\n",
    "    print(\"Veuillez d'abord exécuter le notebook de prétraitement des données.\")\n",
    "    \n",
    "    # Alternative: charger les données depuis les fichiers CSV\n",
    "    try:\n",
    "        train_file = os.path.join(preprocessed_dir, 'train_processed.csv')\n",
    "        test_file = os.path.join(preprocessed_dir, 'test_processed.csv')\n",
    "        \n",
    "        train_data = pd.read_csv(train_file)\n",
    "        X_test = pd.read_csv(test_file)\n",
    "        \n",
    "        # Séparer les caractéristiques et la cible dans les données d'entraînement\n",
    "        y_train = train_data['Survived']\n",
    "        X_train = train_data.drop('Survived', axis=1)\n",
    "        \n",
    "        # Récupérer les IDs des passagers de test depuis le fichier d'origine\n",
    "        test_data_original = pd.read_csv('/workspaces/titanicML/Data/test.csv')\n",
    "        test_passenger_ids = test_data_original['PassengerId']\n",
    "        \n",
    "        feature_names = X_train.columns.tolist()\n",
    "        \n",
    "        print(\"Données chargées depuis les fichiers CSV.\")\n",
    "        print(f\"Ensemble d'entraînement: {X_train.shape[0]} observations, {X_train.shape[1]} caractéristiques\")\n",
    "        print(f\"Ensemble de test: {X_test.shape[0]} observations, {X_test.shape[1]} caractéristiques\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Erreur: Fichiers CSV également introuvables.\")\n",
    "        print(\"Veuillez exécuter le notebook de prétraitement des données avant de continuer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713f9da",
   "metadata": {},
   "source": [
    "## 2. Création d'un ensemble de validation\n",
    "\n",
    "Avant de commencer la modélisation, divisons notre ensemble d'entraînement pour créer un ensemble de validation qui nous permettra d'évaluer les performances de nos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b296c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Ensemble d'entraînement: {X_train_split.shape[0]} observations\")\n",
    "print(f\"Ensemble de validation: {X_val.shape[0]} observations\")\n",
    "\n",
    "# Vérifier la distribution de la variable cible dans les ensembles d'entraînement et de validation\n",
    "print(\"\\nDistribution de la variable cible (Survived):\")\n",
    "print(f\"Ensemble d'entraînement: {y_train_split.mean()*100:.2f}% de survivants\")\n",
    "print(f\"Ensemble de validation: {y_val.mean()*100:.2f}% de survivants\")\n",
    "\n",
    "# Vérifier les dimensions des données\n",
    "print(\"\\nDimensions des ensembles de données:\")\n",
    "print(f\"X_train_split: {X_train_split.shape}\")\n",
    "print(f\"y_train_split: {y_train_split.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16df04",
   "metadata": {},
   "source": [
    "## 3. Évaluation de différents algorithmes de classification\n",
    "\n",
    "Testons plusieurs algorithmes de classification pour identifier les plus prometteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d538d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une fonction pour évaluer un modèle avec validation croisée\n",
    "def evaluate_model(model, X, y, cv=5):\n",
    "    # Évaluer avec validation croisée\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Entraîner le modèle sur l'ensemble complet pour l'analyse des caractéristiques\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Retourner les résultats\n",
    "    return {\n",
    "        'model': model,\n",
    "        'cv_scores': cv_scores,\n",
    "        'mean_cv_score': cv_scores.mean(),\n",
    "        'std_cv_score': cv_scores.std(),\n",
    "        'trained_model': model  # Modèle entraîné sur l'ensemble complet\n",
    "    }\n",
    "\n",
    "# Définir les modèles à évaluer\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=500),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Évaluer chaque modèle avec validation croisée\n",
    "model_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Évaluation de {name}...\")\n",
    "    results = evaluate_model(model, X_train_split, y_train_split, cv=cv)\n",
    "    model_results[name] = results\n",
    "    print(f\"  Précision moyenne (CV): {results['mean_cv_score']:.4f} ± {results['std_cv_score']:.4f}\")\n",
    "\n",
    "# Afficher les résultats sous forme de tableau\n",
    "results_df = pd.DataFrame({\n",
    "    'Modèle': list(model_results.keys()),\n",
    "    'Précision moyenne (CV)': [results['mean_cv_score'] for results in model_results.values()],\n",
    "    'Écart-type': [results['std_cv_score'] for results in model_results.values()]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('Précision moyenne (CV)', ascending=False).reset_index(drop=True)\n",
    "display(results_df)\n",
    "\n",
    "# Visualiser les résultats\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x='Précision moyenne (CV)', y='Modèle', data=results_df, palette='viridis')\n",
    "plt.title('Comparaison des performances des modèles (Validation croisée)', fontsize=16)\n",
    "plt.xlabel('Précision moyenne', fontsize=14)\n",
    "plt.ylabel('Modèle', fontsize=14)\n",
    "\n",
    "# Ajouter les valeurs de précision sur les barres\n",
    "for i, v in enumerate(results_df['Précision moyenne (CV)']):\n",
    "    plt.text(v + 0.01, i, f\"{v:.4f}\", va='center', fontsize=12)\n",
    "\n",
    "plt.xlim(0.75, 0.9)  # Ajuster les limites pour une meilleure visualisation\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c56b3",
   "metadata": {},
   "source": [
    "## 4. Analyse des caractéristiques importantes\n",
    "\n",
    "Analysons l'importance des différentes caractéristiques pour comprendre ce qui influence le plus les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de540b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour extraire l'importance des caractéristiques selon le type de modèle\n",
    "def get_feature_importance(model, feature_names):\n",
    "    if hasattr(model, 'feature_importances_'):  # Pour les modèles basés sur les arbres\n",
    "        return pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    elif hasattr(model, 'coef_'):  # Pour les modèles linéaires\n",
    "        # Utiliser la valeur absolue des coefficients comme mesure d'importance\n",
    "        importances = np.abs(model.coef_[0])\n",
    "        return pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Modèles pour lesquels nous pouvons analyser l'importance des caractéristiques\n",
    "tree_based_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Decision Tree']\n",
    "linear_models = ['Logistic Regression']\n",
    "\n",
    "# Afficher l'importance des caractéristiques pour les modèles pertinents\n",
    "for name in tree_based_models + linear_models:\n",
    "    if name in model_results:\n",
    "        model = model_results[name]['trained_model']\n",
    "        importance_df = get_feature_importance(model, feature_names)\n",
    "        \n",
    "        if importance_df is not None:\n",
    "            print(f\"\\nImportance des caractéristiques pour {name}:\")\n",
    "            display(importance_df.head(15))  # Afficher les 15 caractéristiques les plus importantes\n",
    "            \n",
    "            # Visualiser l'importance des caractéristiques\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=importance_df.head(15), palette='viridis')\n",
    "            plt.title(f'Importance des caractéristiques pour {name}', fontsize=16)\n",
    "            plt.xlabel('Importance', fontsize=14)\n",
    "            plt.ylabel('Caractéristique', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80679d6f",
   "metadata": {},
   "source": [
    "## 5. Optimisation des hyperparamètres des modèles les plus prometteurs\n",
    "\n",
    "Optimisons les hyperparamètres des modèles les plus performants pour améliorer leurs performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd99e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les 3 modèles les plus performants pour l'optimisation\n",
    "top_models = results_df['Modèle'].head(3).tolist()\n",
    "print(f\"Optimisation des hyperparamètres pour les modèles: {', '.join(top_models)}\")\n",
    "\n",
    "# Définir les grilles de paramètres pour l'optimisation\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fonction pour optimiser les hyperparamètres avec RandomizedSearchCV\n",
    "def optimize_hyperparameters(model_name, base_model, param_grid, X, y, cv=5, n_iter=20):\n",
    "    print(f\"\\nOptimisation des hyperparamètres pour {model_name}...\")\n",
    "    \n",
    "    # Utiliser RandomizedSearchCV pour l'efficacité\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    # Afficher les meilleurs paramètres et score\n",
    "    print(f\"Meilleurs paramètres trouvés: {random_search.best_params_}\")\n",
    "    print(f\"Meilleur score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.best_params_, random_search.best_score_\n",
    "\n",
    "# Optimiser les modèles sélectionnés\n",
    "optimized_models = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        base_model = models[model_name]\n",
    "        param_grid = param_grids[model_name]\n",
    "        \n",
    "        best_model, best_params, best_score = optimize_hyperparameters(\n",
    "            model_name, base_model, param_grid, X_train_split, y_train_split, cv=cv\n",
    "        )\n",
    "        \n",
    "        optimized_models[model_name] = {\n",
    "            'model': best_model,\n",
    "            'params': best_params,\n",
    "            'cv_score': best_score\n",
    "        }\n",
    "\n",
    "# Afficher les résultats de l'optimisation\n",
    "results_optimized = pd.DataFrame({\n",
    "    'Modèle': list(optimized_models.keys()),\n",
    "    'Précision CV optimisée': [model_info['cv_score'] for model_info in optimized_models.values()]\n",
    "})\n",
    "\n",
    "display(results_optimized.sort_values('Précision CV optimisée', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e6e76",
   "metadata": {},
   "source": [
    "## 6. Évaluation des modèles optimisés sur l'ensemble de validation\n",
    "\n",
    "Évaluons maintenant les performances des modèles optimisés sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour évaluer un modèle sur l'ensemble de validation\n",
    "def evaluate_on_validation(model, X_train, y_train, X_val, y_val):\n",
    "    # Entraîner le modèle sur l'ensemble d'entraînement\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Faire des prédictions sur l'ensemble de validation\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    # Calculer l'AUC si possible\n",
    "    auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Évaluer chaque modèle optimisé\n",
    "validation_results = {}\n",
    "\n",
    "for model_name, model_info in optimized_models.items():\n",
    "    model = model_info['model']\n",
    "    results = evaluate_on_validation(model, X_train_split, y_train_split, X_val, y_val)\n",
    "    validation_results[model_name] = results\n",
    "    \n",
    "    print(f\"\\nRésultats pour {model_name} sur l'ensemble de validation:\")\n",
    "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "    print(f\"  F1-score: {results['f1']:.4f}\")\n",
    "    if results['auc'] is not None:\n",
    "        print(f\"  AUC: {results['auc']:.4f}\")\n",
    "    \n",
    "    print(\"\\n  Matrice de confusion:\")\n",
    "    cm = results['confusion_matrix']\n",
    "    display(pd.DataFrame(\n",
    "        cm, \n",
    "        index=['Réel: Non-survie (0)', 'Réel: Survie (1)'],\n",
    "        columns=['Prédit: Non-survie (0)', 'Prédit: Survie (1)']\n",
    "    ))\n",
    "\n",
    "# Créer un DataFrame pour comparer les métriques\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Modèle': list(validation_results.keys()),\n",
    "    'Accuracy': [results['accuracy'] for results in validation_results.values()],\n",
    "    'Precision': [results['precision'] for results in validation_results.values()],\n",
    "    'Recall': [results['recall'] for results in validation_results.values()],\n",
    "    'F1-score': [results['f1'] for results in validation_results.values()],\n",
    "    'AUC': [results['auc'] for results in validation_results.values() if results['auc'] is not None]\n",
    "})\n",
    "\n",
    "display(metrics_df.sort_values('Accuracy', ascending=False))\n",
    "\n",
    "# Visualiser les courbes ROC pour les modèles optimisés\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, results in validation_results.items():\n",
    "    if results['y_pred_proba'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_val, results['y_pred_proba'])\n",
    "        auc = results['auc']\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Ligne de référence (aléatoire)\n",
    "plt.xlabel('Taux de faux positifs', fontsize=12)\n",
    "plt.ylabel('Taux de vrais positifs', fontsize=12)\n",
    "plt.title('Courbes ROC des modèles optimisés', fontsize=16)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02762da",
   "metadata": {},
   "source": [
    "## 7. Sélection du modèle final et entraînement sur toutes les données\n",
    "\n",
    "Sélectionnons le meilleur modèle et entraînons-le sur l'ensemble complet des données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner le meilleur modèle en fonction des performances sur l'ensemble de validation\n",
    "best_model_name = metrics_df.iloc[0]['Modèle']\n",
    "best_model = optimized_models[best_model_name]['model']\n",
    "best_params = optimized_models[best_model_name]['params']\n",
    "\n",
    "print(f\"Le meilleur modèle est: {best_model_name}\")\n",
    "print(f\"Paramètres optimaux: {best_params}\")\n",
    "\n",
    "# Entraîner le modèle final sur l'ensemble complet des données d'entraînement\n",
    "print(\"\\nEntraînement du modèle final sur toutes les données d'entraînement...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculer l'importance des caractéristiques si disponible\n",
    "importance_df = get_feature_importance(best_model, feature_names)\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(\"\\nImportance des caractéristiques du modèle final:\")\n",
    "    display(importance_df.head(15))\n",
    "    \n",
    "    # Visualiser l'importance des caractéristiques\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15), palette='viridis')\n",
    "    plt.title(f'Importance des caractéristiques du modèle final ({best_model_name})', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=14)\n",
    "    plt.ylabel('Caractéristique', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39fc185",
   "metadata": {},
   "source": [
    "## 8. Générer les prédictions finales pour l'ensemble de test\n",
    "\n",
    "Utilisons notre modèle final pour faire des prédictions sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14165daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire des prédictions sur l'ensemble de test\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_probabilities = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Créer le DataFrame pour la soumission\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "print(\"Aperçu des prédictions:\")\n",
    "display(submission.head(10))\n",
    "\n",
    "# Distribution des prédictions\n",
    "survived_count = submission['Survived'].sum()\n",
    "total_count = len(submission)\n",
    "survival_rate = survived_count / total_count * 100\n",
    "\n",
    "print(f\"\\nStatistiques des prédictions:\")\n",
    "print(f\"Nombre de passagers prédits comme survivants: {survived_count} sur {total_count} ({survival_rate:.2f}%)\")\n",
    "\n",
    "# Visualiser la distribution des prédictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Survived', data=submission, palette=['#FF5252', '#4CAF50'])\n",
    "plt.title('Distribution des prédictions de survie', fontsize=16)\n",
    "plt.xlabel('Prédit comme survivant (0 = Non, 1 = Oui)')\n",
    "plt.ylabel('Nombre de passagers')\n",
    "\n",
    "# Ajouter les annotations sur les barres\n",
    "counts = submission['Survived'].value_counts()\n",
    "for i, count in enumerate(counts):\n",
    "    percentage = count / total_count * 100\n",
    "    plt.text(i, count + 5, f'{count} ({percentage:.1f}%)', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarder le fichier de soumission\n",
    "submissions_dir = '/workspaces/titanicML/submissions'\n",
    "if not os.path.exists(submissions_dir):\n",
    "    os.makedirs(submissions_dir)\n",
    "\n",
    "submission_file = os.path.join(submissions_dir, f'submission_{best_model_name.replace(\" \", \"_\").lower()}.csv')\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"\\nFichier de soumission créé: {submission_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375fef4",
   "metadata": {},
   "source": [
    "## 9. Sauvegarder le modèle final\n",
    "\n",
    "Sauvegardons le modèle final pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95541ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire avec le modèle et les métadonnées\n",
    "final_model_data = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_model_name,\n",
    "    'params': best_params,\n",
    "    'feature_names': feature_names,\n",
    "    'scaler': scaler,  # Le scaler utilisé pour normaliser les données\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Créer le dossier pour les modèles s'il n'existe pas\n",
    "models_dir = '/workspaces/titanicML/models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model_file = os.path.join(models_dir, f'final_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl')\n",
    "joblib.dump(final_model_data, model_file)\n",
    "\n",
    "print(f\"Modèle final sauvegardé: {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e86653",
   "metadata": {},
   "source": [
    "## 10. Conclusions et prochaines étapes\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "\n",
    "1. **Testé plusieurs algorithmes de classification**:\n",
    "   - Régression logistique\n",
    "   - Arbre de décision\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "   - SVM\n",
    "   - K-Nearest Neighbors\n",
    "   - XGBoost\n",
    "\n",
    "2. **Optimisé les hyperparamètres** des modèles les plus prometteurs pour améliorer leurs performances\n",
    "\n",
    "3. **Évalué les performances des modèles** sur un ensemble de validation à l'aide de plusieurs métriques:\n",
    "   - Accuracy\n",
    "   - Precision\n",
    "   - Recall\n",
    "   - F1-score\n",
    "   - AUC-ROC\n",
    "\n",
    "4. **Sélectionné le meilleur modèle** en fonction de ses performances et l'avons entraîné sur toutes les données d'entraînement\n",
    "\n",
    "5. **Généré des prédictions** pour l'ensemble de test et créé un fichier de soumission pour Kaggle\n",
    "\n",
    "### Prochaines étapes:\n",
    "- **Soumettre nos prédictions à Kaggle** et évaluer notre score\n",
    "- **Affiner davantage le modèle** en créant de nouvelles caractéristiques ou en essayant des techniques d'ensemble\n",
    "- **Explorer des approches de stacking** en combinant plusieurs modèles pour améliorer les performances\n",
    "- **Analyser les erreurs** pour comprendre où notre modèle échoue et comment l'améliorer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
