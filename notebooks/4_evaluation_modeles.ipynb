{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ce8860",
   "metadata": {},
   "source": [
    "# Évaluation des Modèles - Challenge Titanic\n",
    "\n",
    "Ce notebook se concentre sur l'évaluation approfondie des modèles de prédiction pour le challenge Titanic. Nous allons analyser les performances de nos modèles, comprendre leurs forces et faiblesses, et identifier les opportunités d'amélioration.\n",
    "\n",
    "## Objectifs de l'évaluation:\n",
    "- Analyser les performances du modèle sur différents segments de données\n",
    "- Identifier les points forts et les faiblesses des modèles\n",
    "- Examiner les erreurs de prédiction pour comprendre où le modèle échoue\n",
    "- Comparer différentes métriques d'évaluation\n",
    "- Explorer des techniques d'ensemble pour améliorer les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour l'évaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "\n",
    "# Pour les techniques d'ensemble\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "# Pour charger les données et les modèles\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Pour afficher les graphiques dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Pour afficher toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ad046",
   "metadata": {},
   "source": [
    "## 1. Chargement des données et des modèles\n",
    "\n",
    "Commençons par charger les données prétraitées et les modèles que nous avons entraînés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données prétraitées\n",
    "preprocessed_dir = '/workspaces/titanicML/Data/preprocessed'\n",
    "preprocessed_file = os.path.join(preprocessed_dir, 'preprocessed_data.pkl')\n",
    "\n",
    "try:\n",
    "    # Charger les données prétraitées\n",
    "    preprocessed_data = joblib.load(preprocessed_file)\n",
    "    \n",
    "    # Extraire les ensembles de données\n",
    "    X_train = preprocessed_data['train_features']\n",
    "    y_train = preprocessed_data['train_target']\n",
    "    X_test = preprocessed_data['test_features']\n",
    "    test_passenger_ids = preprocessed_data['test_passenger_ids']\n",
    "    feature_names = preprocessed_data['feature_names']\n",
    "    \n",
    "    print(\"Données prétraitées chargées avec succès!\")\n",
    "    print(f\"Ensemble d'entraînement: {X_train.shape[0]} observations, {X_train.shape[1]} caractéristiques\")\n",
    "    print(f\"Ensemble de test: {X_test.shape[0]} observations, {X_test.shape[1]} caractéristiques\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur: Fichier {preprocessed_file} introuvable.\")\n",
    "    print(\"Alternative: chargement des fichiers CSV...\")\n",
    "    \n",
    "    try:\n",
    "        train_file = os.path.join(preprocessed_dir, 'train_processed.csv')\n",
    "        test_file = os.path.join(preprocessed_dir, 'test_processed.csv')\n",
    "        \n",
    "        train_data = pd.read_csv(train_file)\n",
    "        X_test = pd.read_csv(test_file)\n",
    "        \n",
    "        # Séparer les caractéristiques et la cible dans les données d'entraînement\n",
    "        y_train = train_data['Survived']\n",
    "        X_train = train_data.drop('Survived', axis=1)\n",
    "        \n",
    "        # Récupérer les IDs des passagers de test depuis le fichier d'origine\n",
    "        test_data_original = pd.read_csv('/workspaces/titanicML/Data/test.csv')\n",
    "        test_passenger_ids = test_data_original['PassengerId']\n",
    "        \n",
    "        feature_names = X_train.columns.tolist()\n",
    "        \n",
    "        print(\"Données chargées depuis les fichiers CSV.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Erreur: Fichiers CSV également introuvables.\")\n",
    "        print(\"Veuillez exécuter le notebook de prétraitement des données avant de continuer.\")\n",
    "\n",
    "# Charger les modèles entraînés\n",
    "models_dir = '/workspaces/titanicML/models'\n",
    "model_files = glob.glob(os.path.join(models_dir, 'final_model_*.pkl'))\n",
    "\n",
    "if model_files:\n",
    "    print(f\"\\nModèles trouvés: {len(model_files)}\")\n",
    "    \n",
    "    # Charger chaque modèle\n",
    "    models = {}\n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model_name = os.path.basename(model_file).replace('final_model_', '').replace('.pkl', '').replace('_', ' ')\n",
    "            model_data = joblib.load(model_file)\n",
    "            models[model_name] = model_data\n",
    "            print(f\"  - {model_name} chargé avec succès\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Erreur lors du chargement de {model_file}: {e}\")\n",
    "    \n",
    "    # Vérifier si au moins un modèle a été chargé\n",
    "    if models:\n",
    "        print(\"\\nModèles chargés avec succès!\")\n",
    "    else:\n",
    "        print(\"\\nAucun modèle n'a pu être chargé.\")\n",
    "else:\n",
    "    print(\"\\nAucun modèle trouvé dans le dossier des modèles.\")\n",
    "    print(\"Veuillez exécuter le notebook de modélisation avant de continuer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b8de4",
   "metadata": {},
   "source": [
    "## 2. Division des données pour l'évaluation\n",
    "\n",
    "Pour une évaluation plus rigoureuse, nous allons diviser les données d'entraînement en ensembles d'entraînement et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56556b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données pour l'évaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Ensemble d'entraînement: {X_train_split.shape[0]} observations\")\n",
    "print(f\"Ensemble de validation: {X_val.shape[0]} observations\")\n",
    "\n",
    "# Vérifier la distribution de la variable cible\n",
    "print(\"\\nDistribution de la variable cible (Survived):\")\n",
    "print(f\"Ensemble d'entraînement: {y_train_split.mean()*100:.2f}% de survivants\")\n",
    "print(f\"Ensemble de validation: {y_val.mean()*100:.2f}% de survivants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5773b",
   "metadata": {},
   "source": [
    "## 3. Évaluation détaillée des modèles\n",
    "\n",
    "Analysons en détail les performances de chaque modèle sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e03829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour évaluer un modèle en détail\n",
    "def evaluate_model_in_detail(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    # S'assurer que le modèle est entraîné sur les données d'entraînement\n",
    "    if hasattr(model, 'fit'):\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Faire des prédictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Métriques de base\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    # Métriques avancées\n",
    "    if y_pred_proba is not None:\n",
    "        auc = roc_auc_score(y_val, y_pred_proba)\n",
    "        avg_precision = average_precision_score(y_val, y_pred_proba)\n",
    "        log_loss_val = log_loss(y_val, y_pred_proba)\n",
    "        brier = brier_score_loss(y_val, y_pred_proba)\n",
    "    else:\n",
    "        auc = None\n",
    "        avg_precision = None\n",
    "        log_loss_val = None\n",
    "        brier = None\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"\\n=== Évaluation détaillée de {model_name} ===\")\n",
    "    print(\"\\nMétriques de classification:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-score: {f1:.4f}\")\n",
    "    \n",
    "    if auc is not None:\n",
    "        print(\"\\nMétriques de probabilité:\")\n",
    "        print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "        print(f\"  Précision moyenne: {avg_precision:.4f}\")\n",
    "        print(f\"  Log Loss: {log_loss_val:.4f}\")\n",
    "        print(f\"  Brier Score: {brier:.4f}\")\n",
    "    \n",
    "    # Afficher la matrice de confusion\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Prédit: Non-survie', 'Prédit: Survie'],\n",
    "               yticklabels=['Réel: Non-survie', 'Réel: Survie'])\n",
    "    plt.title(f'Matrice de confusion - {model_name}', fontsize=16)\n",
    "    plt.ylabel('Valeur réelle', fontsize=12)\n",
    "    plt.xlabel('Valeur prédite', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher le rapport de classification\n",
    "    print(\"\\nRapport de classification:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=['Non-survie', 'Survie']))\n",
    "    \n",
    "    # Tracer la courbe ROC si disponible\n",
    "    if y_pred_proba is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Aléatoire')\n",
    "        plt.xlabel('Taux de faux positifs', fontsize=12)\n",
    "        plt.ylabel('Taux de vrais positifs', fontsize=12)\n",
    "        plt.title(f'Courbe ROC - {model_name}', fontsize=16)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Tracer la courbe Precision-Recall\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "        plt.plot(recall_vals, precision_vals, label=f'AP = {avg_precision:.4f}')\n",
    "        plt.axhline(y=y_val.mean(), color='r', linestyle='--', label='Aléatoire')\n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title(f'Courbe Precision-Recall - {model_name}', fontsize=16)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'avg_precision': avg_precision,\n",
    "        'log_loss': log_loss_val,\n",
    "        'brier_score': brier,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Évaluer chaque modèle chargé\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    results = evaluate_model_in_detail(model, X_train_split, y_train_split, X_val, y_val, model_name)\n",
    "    evaluation_results[model_name] = results\n",
    "\n",
    "# Comparer les modèles sur les métriques principales\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Modèle': list(evaluation_results.keys()),\n",
    "    'Accuracy': [results['accuracy'] for results in evaluation_results.values()],\n",
    "    'Precision': [results['precision'] for results in evaluation_results.values()],\n",
    "    'Recall': [results['recall'] for results in evaluation_results.values()],\n",
    "    'F1-score': [results['f1'] for results in evaluation_results.values()]\n",
    "})\n",
    "\n",
    "# Ajouter les métriques de probabilité si disponibles\n",
    "if all(results['auc'] is not None for results in evaluation_results.values()):\n",
    "    metrics_df['AUC-ROC'] = [results['auc'] for results in evaluation_results.values()]\n",
    "    metrics_df['Précision moyenne'] = [results['avg_precision'] for results in evaluation_results.values()]\n",
    "    metrics_df['Log Loss'] = [results['log_loss'] for results in evaluation_results.values()]\n",
    "    metrics_df['Brier Score'] = [results['brier_score'] for results in evaluation_results.values()]\n",
    "\n",
    "# Afficher le tableau comparatif\n",
    "display(metrics_df.sort_values('F1-score', ascending=False))\n",
    "\n",
    "# Visualiser la comparaison des métriques principales\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "metrics_df_melted = pd.melt(metrics_df, id_vars=['Modèle'], value_vars=metrics_to_plot,\n",
    "                          var_name='Métrique', value_name='Valeur')\n",
    "\n",
    "sns.barplot(x='Métrique', y='Valeur', hue='Modèle', data=metrics_df_melted)\n",
    "plt.title('Comparaison des performances des modèles', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.ylim(0.7, 1.0)  # Ajuster selon vos résultats\n",
    "plt.legend(title='Modèle')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd479a5d",
   "metadata": {},
   "source": [
    "## 4. Analyse des erreurs de prédiction\n",
    "\n",
    "Examinons en détail où et pourquoi nos modèles font des erreurs de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner le meilleur modèle pour l'analyse des erreurs\n",
    "if evaluation_results:\n",
    "    best_model_name = metrics_df.sort_values('F1-score', ascending=False).iloc[0]['Modèle']\n",
    "    best_model_results = evaluation_results[best_model_name]\n",
    "    best_model = models[best_model_name]['model']\n",
    "    print(f\"Analyse des erreurs pour le meilleur modèle: {best_model_name}\")\n",
    "else:\n",
    "    print(\"Aucun modèle disponible pour l'analyse des erreurs.\")\n",
    "    # Si aucun modèle n'est disponible, utiliser un modèle de secours\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    best_model = RandomForestClassifier(random_state=42)\n",
    "    best_model.fit(X_train_split, y_train_split)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "    best_model_name = \"Random Forest (secours)\"\n",
    "\n",
    "# Fonction pour analyser les erreurs\n",
    "def analyze_prediction_errors(model, X_val, y_val, feature_names):\n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Créer un DataFrame pour l'analyse\n",
    "    val_data = X_val.copy()\n",
    "    val_data['Survived_Actual'] = y_val\n",
    "    val_data['Survived_Predicted'] = y_pred\n",
    "    if y_pred_proba is not None:\n",
    "        val_data['Survival_Probability'] = y_pred_proba\n",
    "    \n",
    "    # Identifier les erreurs\n",
    "    val_data['Correct_Prediction'] = (val_data['Survived_Actual'] == val_data['Survived_Predicted']).astype(int)\n",
    "    val_data['Error_Type'] = 'Correct'\n",
    "    val_data.loc[(val_data['Survived_Actual'] == 1) & (val_data['Survived_Predicted'] == 0), 'Error_Type'] = 'Faux Négatif'\n",
    "    val_data.loc[(val_data['Survived_Actual'] == 0) & (val_data['Survived_Predicted'] == 1), 'Error_Type'] = 'Faux Positif'\n",
    "    \n",
    "    # Afficher la distribution des erreurs\n",
    "    error_counts = val_data['Error_Type'].value_counts()\n",
    "    print(\"\\nDistribution des prédictions:\")\n",
    "    print(error_counts)\n",
    "    \n",
    "    # Visualiser la distribution des erreurs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='Error_Type', data=val_data, palette={'Correct': '#4CAF50', 'Faux Négatif': '#FF9800', 'Faux Positif': '#F44336'})\n",
    "    plt.title('Distribution des erreurs de prédiction', fontsize=16)\n",
    "    plt.xlabel('Type d\\'erreur', fontsize=12)\n",
    "    plt.ylabel('Nombre d\\'observations', fontsize=12)\n",
    "    \n",
    "    # Ajouter les pourcentages sur les barres\n",
    "    for i, count in enumerate(error_counts):\n",
    "        percentage = count / len(val_data) * 100\n",
    "        plt.text(i, count + 5, f'{count} ({percentage:.1f}%)', ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Analyser les erreurs par caractéristiques clés\n",
    "    if 'Sex_Code' in val_data.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.countplot(x='Sex_Code', hue='Error_Type', data=val_data, \n",
    "                     palette={'Correct': '#4CAF50', 'Faux Négatif': '#FF9800', 'Faux Positif': '#F44336'})\n",
    "        plt.title('Erreurs par sexe')\n",
    "        plt.xlabel('Sexe (0 = Homme, 1 = Femme)')\n",
    "        plt.ylabel('Nombre d\\'observations')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sex_error_rate = val_data.groupby('Sex_Code')['Correct_Prediction'].mean() * 100\n",
    "        sns.barplot(x=sex_error_rate.index, y=sex_error_rate.values)\n",
    "        plt.title('Taux de prédiction correcte par sexe')\n",
    "        plt.xlabel('Sexe (0 = Homme, 1 = Femme)')\n",
    "        plt.ylabel('Taux de prédiction correcte (%)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyse par classe\n",
    "    if 'Pclass' in val_data.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.countplot(x='Pclass', hue='Error_Type', data=val_data,\n",
    "                     palette={'Correct': '#4CAF50', 'Faux Négatif': '#FF9800', 'Faux Positif': '#F44336'})\n",
    "        plt.title('Erreurs par classe')\n",
    "        plt.xlabel('Classe')\n",
    "        plt.ylabel('Nombre d\\'observations')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        class_error_rate = val_data.groupby('Pclass')['Correct_Prediction'].mean() * 100\n",
    "        sns.barplot(x=class_error_rate.index, y=class_error_rate.values)\n",
    "        plt.title('Taux de prédiction correcte par classe')\n",
    "        plt.xlabel('Classe')\n",
    "        plt.ylabel('Taux de prédiction correcte (%)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyse par âge (si disponible)\n",
    "    if 'Age' in val_data.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='Error_Type', y='Age', data=val_data,\n",
    "                   palette={'Correct': '#4CAF50', 'Faux Négatif': '#FF9800', 'Faux Positif': '#F44336'})\n",
    "        plt.title('Distribution des erreurs par âge', fontsize=16)\n",
    "        plt.xlabel('Type d\\'erreur', fontsize=12)\n",
    "        plt.ylabel('Âge', fontsize=12)\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyse des cas difficiles (probabilités proches de 0.5)\n",
    "    if 'Survival_Probability' in val_data.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=val_data, x='Survival_Probability', hue='Error_Type', bins=30, \n",
    "                    palette={'Correct': '#4CAF50', 'Faux Négatif': '#FF9800', 'Faux Positif': '#F44336'})\n",
    "        plt.title('Distribution des probabilités par type d\\'erreur', fontsize=16)\n",
    "        plt.xlabel('Probabilité prédite de survie', fontsize=12)\n",
    "        plt.ylabel('Nombre d\\'observations', fontsize=12)\n",
    "        plt.axvline(x=0.5, color='black', linestyle='--', label='Seuil de décision')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Identifier les cas les plus difficiles\n",
    "    if 'Survival_Probability' in val_data.columns:\n",
    "        # Calculer la distance au seuil de décision (0.5)\n",
    "        val_data['Threshold_Distance'] = abs(val_data['Survival_Probability'] - 0.5)\n",
    "        \n",
    "        # Filtrer les prédictions incorrectes\n",
    "        incorrect_predictions = val_data[val_data['Correct_Prediction'] == 0]\n",
    "        \n",
    "        # Trier par proximité au seuil\n",
    "        difficult_cases = incorrect_predictions.sort_values('Threshold_Distance').head(10)\n",
    "        \n",
    "        print(\"\\nLes 10 cas les plus difficiles (prédictions incorrectes proches du seuil):\")\n",
    "        display(difficult_cases[['Survived_Actual', 'Survived_Predicted', 'Survival_Probability', 'Error_Type', 'Threshold_Distance'] + \n",
    "                             [col for col in val_data.columns if col in ['Pclass', 'Sex_Code', 'Age', 'Fare', 'FamilySize']]])\n",
    "    \n",
    "    return val_data\n",
    "\n",
    "# Analyser les erreurs du meilleur modèle\n",
    "error_analysis_data = analyze_prediction_errors(best_model, X_val, y_val, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbeaa7",
   "metadata": {},
   "source": [
    "## 5. Courbes d'apprentissage\n",
    "\n",
    "Analysons comment les performances du modèle évoluent en fonction de la taille de l'ensemble d'entraînement pour détecter d'éventuels problèmes de sur-ajustement ou de sous-ajustement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a24099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'apprentissage pour le meilleur modèle\n",
    "def plot_learning_curve(estimator, X, y, title=\"Courbe d'apprentissage\", ylim=None, cv=5,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title, fontsize=16)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Taille de l'ensemble d'entraînement\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Tracer le score sur l'ensemble d'entraînement\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9800\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9800\",\n",
    "             label=\"Score d'entraînement\")\n",
    "    \n",
    "    # Tracer le score sur l'ensemble de validation\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2196f3\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#2196f3\",\n",
    "             label=\"Score de validation\")\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    # Ajouter des lignes horizontales pour le score de référence\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', label=\"Baseline (50%)\")\n",
    "    plt.axhline(y=y.mean(), color='g', linestyle='--', label=f\"Baseline (taux de survie: {y.mean():.2f})\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_scores_mean, test_scores_mean\n",
    "\n",
    "# Tracer la courbe d'apprentissage pour le meilleur modèle\n",
    "print(f\"\\nCourbe d'apprentissage pour {best_model_name}:\")\n",
    "train_scores, test_scores = plot_learning_curve(\n",
    "    best_model, X_train, y_train, \n",
    "    title=f\"Courbe d'apprentissage - {best_model_name}\", \n",
    "    cv=5, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7169dd",
   "metadata": {},
   "source": [
    "## 6. Techniques d'ensemble pour améliorer les performances\n",
    "\n",
    "Explorons les techniques d'ensemble pour combiner les prédictions de différents modèles et potentiellement améliorer les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c291ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si nous avons plusieurs modèles pour créer un ensemble\n",
    "if len(models) >= 2:\n",
    "    print(\"Création de modèles d'ensemble à partir des modèles disponibles...\")\n",
    "    \n",
    "    # Créer une liste des modèles à utiliser\n",
    "    ensemble_models = [(name, model_data['model']) for name, model_data in models.items()]\n",
    "    \n",
    "    # Créer un modèle de vote (soft voting)\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=ensemble_models,\n",
    "        voting='soft'  # Utiliser le vote pondéré par les probabilités\n",
    "    )\n",
    "    \n",
    "    # Entraîner le modèle de vote\n",
    "    voting_classifier.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Évaluer le modèle de vote\n",
    "    voting_results = evaluate_model_in_detail(\n",
    "        voting_classifier, X_train_split, y_train_split, X_val, y_val, \"Voting Classifier (soft)\"\n",
    "    )\n",
    "    \n",
    "    # Créer un modèle de stacking\n",
    "    stacking_classifier = StackingClassifier(\n",
    "        estimators=ensemble_models,\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    # Entraîner le modèle de stacking\n",
    "    stacking_classifier.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Évaluer le modèle de stacking\n",
    "    stacking_results = evaluate_model_in_detail(\n",
    "        stacking_classifier, X_train_split, y_train_split, X_val, y_val, \"Stacking Classifier\"\n",
    "    )\n",
    "    \n",
    "    # Comparer les performances\n",
    "    ensemble_comparison = pd.DataFrame({\n",
    "        'Modèle': [best_model_name, \"Voting Classifier\", \"Stacking Classifier\"],\n",
    "        'Accuracy': [best_model_results['accuracy'], voting_results['accuracy'], stacking_results['accuracy']],\n",
    "        'F1-score': [best_model_results['f1'], voting_results['f1'], stacking_results['f1']],\n",
    "        'AUC-ROC': [best_model_results['auc'], voting_results['auc'], stacking_results['auc']]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nComparaison des performances des modèles d'ensemble:\")\n",
    "    display(ensemble_comparison.sort_values('F1-score', ascending=False))\n",
    "    \n",
    "    # Si le modèle d'ensemble est meilleur, l'utiliser pour les prédictions finales\n",
    "    best_ensemble_model = None\n",
    "    if ensemble_comparison.iloc[0]['Modèle'] == \"Voting Classifier\":\n",
    "        best_ensemble_model = voting_classifier\n",
    "        print(\"\\nLe modèle de vote (Voting Classifier) est le plus performant!\")\n",
    "    elif ensemble_comparison.iloc[0]['Modèle'] == \"Stacking Classifier\":\n",
    "        best_ensemble_model = stacking_classifier\n",
    "        print(\"\\nLe modèle de stacking (Stacking Classifier) est le plus performant!\")\n",
    "    \n",
    "    # Si un modèle d'ensemble est meilleur, l'utiliser pour les prédictions finales\n",
    "    if best_ensemble_model is not None and ensemble_comparison.iloc[0]['F1-score'] > best_model_results['f1']:\n",
    "        print(\"Utilisation du modèle d'ensemble pour les prédictions finales...\")\n",
    "        \n",
    "        # Faire des prédictions sur l'ensemble de test\n",
    "        ensemble_test_predictions = best_ensemble_model.predict(X_test)\n",
    "        \n",
    "        # Créer le DataFrame pour la soumission\n",
    "        ensemble_submission = pd.DataFrame({\n",
    "            'PassengerId': test_passenger_ids,\n",
    "            'Survived': ensemble_test_predictions\n",
    "        })\n",
    "        \n",
    "        # Sauvegarder le fichier de soumission\n",
    "        submissions_dir = '/workspaces/titanicML/submissions'\n",
    "        if not os.path.exists(submissions_dir):\n",
    "            os.makedirs(submissions_dir)\n",
    "        \n",
    "        ensemble_submission_file = os.path.join(submissions_dir, 'submission_ensemble_model.csv')\n",
    "        ensemble_submission.to_csv(ensemble_submission_file, index=False)\n",
    "        \n",
    "        print(f\"Fichier de soumission du modèle d'ensemble créé: {ensemble_submission_file}\")\n",
    "        \n",
    "        # Sauvegarder le modèle d'ensemble\n",
    "        ensemble_model_data = {\n",
    "            'model': best_ensemble_model,\n",
    "            'model_name': ensemble_comparison.iloc[0]['Modèle'],\n",
    "            'feature_names': feature_names,\n",
    "            'base_models': ensemble_models,\n",
    "            'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        ensemble_model_file = os.path.join(models_dir, 'final_model_ensemble.pkl')\n",
    "        joblib.dump(ensemble_model_data, ensemble_model_file)\n",
    "        \n",
    "        print(f\"Modèle d'ensemble sauvegardé: {ensemble_model_file}\")\n",
    "else:\n",
    "    print(\"Pas assez de modèles disponibles pour créer un ensemble.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a089c67",
   "metadata": {},
   "source": [
    "## 7. Conclusions et recommandations\n",
    "\n",
    "Résumons les résultats de notre évaluation et proposons des recommandations pour améliorer les performances des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5691ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter les informations clés pour le résumé\n",
    "if evaluation_results:\n",
    "    # Trouver le meilleur modèle\n",
    "    best_model_row = metrics_df.sort_values('F1-score', ascending=False).iloc[0]\n",
    "    best_model_name = best_model_row['Modèle']\n",
    "    best_f1 = best_model_row['F1-score']\n",
    "    best_accuracy = best_model_row['Accuracy']\n",
    "    \n",
    "    # Caractéristiques importantes (si disponibles)\n",
    "    feature_importance = None\n",
    "    if hasattr(models[best_model_name]['model'], 'feature_importances_'):\n",
    "        importances = models[best_model_name]['model'].feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(10)\n",
    "    elif hasattr(models[best_model_name]['model'], 'coef_'):\n",
    "        importances = np.abs(models[best_model_name]['model'].coef_[0])\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    # Analyser les erreurs fréquentes\n",
    "    if 'Error_Type' in error_analysis_data.columns:\n",
    "        error_counts = error_analysis_data['Error_Type'].value_counts()\n",
    "        error_rate = (error_counts['Faux Négatif'] + error_counts['Faux Positif']) / len(error_analysis_data)\n",
    "        fn_rate = error_counts['Faux Négatif'] / (error_counts['Faux Négatif'] + error_counts.get('Correct', 0))\n",
    "        fp_rate = error_counts['Faux Positif'] / (error_counts['Faux Positif'] + error_counts.get('Correct', 0))\n",
    "    \n",
    "    # Afficher le résumé\n",
    "    print(\"\\n=== RÉSUMÉ DE L'ÉVALUATION DES MODÈLES ===\")\n",
    "    print(f\"\\n1. Meilleur modèle: {best_model_name}\")\n",
    "    print(f\"   - Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"   - F1-score: {best_f1:.4f}\")\n",
    "    \n",
    "    if feature_importance is not None:\n",
    "        print(\"\\n2. Caractéristiques les plus importantes:\")\n",
    "        for i, (feature, importance) in enumerate(zip(feature_importance['Feature'], feature_importance['Importance']), 1):\n",
    "            print(f\"   {i}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    print(\"\\n3. Analyse des erreurs:\")\n",
    "    print(f\"   - Taux d'erreur global: {error_rate:.4f} ({error_rate*100:.1f}%)\")\n",
    "    print(f\"   - Taux de faux négatifs (survies manquées): {fn_rate:.4f} ({fn_rate*100:.1f}%)\")\n",
    "    print(f\"   - Taux de faux positifs (décès manqués): {fp_rate:.4f} ({fp_rate*100:.1f}%)\")\n",
    "    \n",
    "    # Tracer la matrice de confusion pour le résumé\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = best_model_results['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Prédit: Non-survie', 'Prédit: Survie'],\n",
    "               yticklabels=['Réel: Non-survie', 'Réel: Survie'])\n",
    "    plt.title(f'Matrice de confusion - {best_model_name}', fontsize=16)\n",
    "    plt.ylabel('Valeur réelle', fontsize=12)\n",
    "    plt.xlabel('Valeur prédite', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommandations\n",
    "    print(\"\\n=== RECOMMANDATIONS POUR AMÉLIORER LES PERFORMANCES ===\")\n",
    "    print(\"\\n1. Collecte de données supplémentaires:\")\n",
    "    print(\"   - Rechercher des informations sur la disposition des cabines et leur proximité avec les canots de sauvetage\")\n",
    "    print(\"   - Intégrer des données sur les liens familiaux entre passagers\")\n",
    "    \n",
    "    print(\"\\n2. Ingénierie de caractéristiques:\")\n",
    "    print(\"   - Créer des caractéristiques plus granulaires pour l'âge et les tarifs\")\n",
    "    print(\"   - Explorer des interactions entre variables (ex: âge × classe)\")\n",
    "    print(\"   - Extraire plus d'informations des noms (nationalité, etc.)\")\n",
    "    \n",
    "    print(\"\\n3. Optimisation des modèles:\")\n",
    "    print(\"   - Tester des techniques d'ensemble plus avancées\")\n",
    "    print(\"   - Optimiser les seuils de décision pour réduire les faux négatifs ou les faux positifs selon l'objectif\")\n",
    "    print(\"   - Explorer des réseaux de neurones profonds avec une architecture adaptée\")\n",
    "    \n",
    "    print(\"\\n4. Validation et généralisation:\")\n",
    "    print(\"   - Effectuer une validation croisée plus robuste avec plus de folds\")\n",
    "    print(\"   - Tester la robustesse du modèle sur différents échantillons des données\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca39a93",
   "metadata": {},
   "source": [
    "## 8. Prochaines étapes\n",
    "\n",
    "Pour finaliser notre projet sur le challenge Titanic, nous devrons:\n",
    "\n",
    "1. **Sélectionner le modèle final** en fonction des résultats de cette évaluation approfondie\n",
    "\n",
    "2. **Générer les prédictions finales** pour l'ensemble de test avec le meilleur modèle (individuel ou d'ensemble)\n",
    "\n",
    "3. **Préparer le fichier de soumission** au format requis par Kaggle\n",
    "\n",
    "4. **Soumettre nos prédictions** sur la plateforme Kaggle et évaluer notre score\n",
    "\n",
    "5. **Documenter le processus complet** de notre approche, y compris:\n",
    "   - Prétraitement des données\n",
    "   - Création de caractéristiques\n",
    "   - Sélection et optimisation des modèles\n",
    "   - Résultats d'évaluation\n",
    "   - Leçons apprises et améliorations potentielles\n",
    "\n",
    "La dernière étape de notre projet consistera à créer le fichier de soumission final et à le soumettre à Kaggle."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
