{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b404c1",
   "metadata": {},
   "source": [
    "# Prétraitement des Données - Challenge Titanic\n",
    "\n",
    "Ce notebook vous guide à travers les étapes essentielles de prétraitement des données pour le challenge Titanic. L'objectif est de transformer les données brutes en caractéristiques exploitables par les algorithmes de machine learning.\n",
    "\n",
    "## Objectifs du prétraitement:\n",
    "- Gérer les valeurs manquantes de manière appropriée\n",
    "- Créer de nouvelles caractéristiques pertinentes\n",
    "- Encoder les variables catégorielles\n",
    "- Préparer les données pour l'entraînement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4289c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour le prétraitement\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Pour afficher les graphiques dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Pour afficher toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333fad53",
   "metadata": {},
   "source": [
    "## 1. Chargement des données\n",
    "\n",
    "Commençons par charger les données d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "train_data = pd.read_csv('/workspaces/titanicML/Data/train.csv')\n",
    "test_data = pd.read_csv('/workspaces/titanicML/Data/test.csv')\n",
    "\n",
    "# Sauvegarde des identifiants de passagers pour la soumission finale\n",
    "test_passenger_ids = test_data['PassengerId']\n",
    "\n",
    "# Aperçu des données\n",
    "print(f\"Données d'entraînement: {train_data.shape[0]} lignes et {train_data.shape[1]} colonnes\")\n",
    "print(f\"Données de test: {test_data.shape[0]} lignes et {test_data.shape[1]} colonnes\")\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "missing_train = train_data.isnull().sum()\n",
    "missing_test = test_data.isnull().sum()\n",
    "\n",
    "missing_train_percent = (missing_train / len(train_data)) * 100\n",
    "missing_test_percent = (missing_test / len(test_data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Train - Valeurs manquantes': missing_train,\n",
    "    'Train - Pourcentage (%)': missing_train_percent,\n",
    "    'Test - Valeurs manquantes': missing_test,\n",
    "    'Test - Pourcentage (%)': missing_test_percent\n",
    "})\n",
    "\n",
    "display(missing_df[(missing_df['Train - Valeurs manquantes'] > 0) | \n",
    "                 (missing_df['Test - Valeurs manquantes'] > 0)].sort_values('Train - Pourcentage (%)', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a09bf",
   "metadata": {},
   "source": [
    "## 2. Combinaison temporaire des ensembles de données pour le prétraitement\n",
    "\n",
    "Pour assurer une cohérence dans le prétraitement, nous allons combiner temporairement les ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les variables cibles avant la combinaison\n",
    "train_targets = train_data['Survived']\n",
    "\n",
    "# Marquer les données d'entraînement et de test pour pouvoir les séparer plus tard\n",
    "train_data['is_train'] = 1\n",
    "test_data['is_train'] = 0\n",
    "\n",
    "# Combiner les données pour le prétraitement\n",
    "combined_data = pd.concat([train_data.drop('Survived', axis=1), test_data], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dimensions des données combinées: {combined_data.shape[0]} lignes et {combined_data.shape[1]} colonnes\")\n",
    "print(f\"Dont {train_data.shape[0]} lignes d'entraînement et {test_data.shape[0]} lignes de test\")\n",
    "\n",
    "# Aperçu des données combinées\n",
    "display(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa829347",
   "metadata": {},
   "source": [
    "## 3. Gestion des valeurs manquantes\n",
    "\n",
    "### 3.1 Imputation de l'âge manquant\n",
    "\n",
    "Pour imputer les âges manquants, nous allons utiliser une approche basée sur le titre et la classe du passager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b78a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction du titre à partir du nom\n",
    "combined_data['Title'] = combined_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Afficher les titres uniques\n",
    "print(\"Titres uniques trouvés:\", combined_data['Title'].unique())\n",
    "print(\"\\nFréquence des titres:\")\n",
    "display(combined_data['Title'].value_counts())\n",
    "\n",
    "# Regrouper les titres moins fréquents\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr',\n",
    "    'Miss': 'Miss',\n",
    "    'Mrs': 'Mrs',\n",
    "    'Master': 'Master',\n",
    "    'Dr': 'Rare',\n",
    "    'Rev': 'Rare',\n",
    "    'Col': 'Rare',\n",
    "    'Major': 'Rare',\n",
    "    'Mlle': 'Miss',\n",
    "    'Countess': 'Rare',\n",
    "    'Ms': 'Miss',\n",
    "    'Lady': 'Rare',\n",
    "    'Jonkheer': 'Rare',\n",
    "    'Don': 'Rare',\n",
    "    'Dona': 'Rare',\n",
    "    'Mme': 'Mrs',\n",
    "    'Capt': 'Rare',\n",
    "    'Sir': 'Rare'\n",
    "}\n",
    "\n",
    "combined_data['Title'] = combined_data['Title'].map(title_mapping)\n",
    "\n",
    "# Vérifier la distribution des titres après regroupement\n",
    "display(combined_data['Title'].value_counts())\n",
    "\n",
    "# Calculer l'âge médian par titre et classe\n",
    "age_by_title_class = combined_data.groupby(['Title', 'Pclass'])['Age'].median()\n",
    "print(\"\\nÂge médian par titre et classe:\")\n",
    "display(age_by_title_class)\n",
    "\n",
    "# Fonction pour imputer l'âge manquant\n",
    "def impute_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        return age_by_title_class[(row['Title'], row['Pclass'])]\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "# Appliquer la fonction d'imputation\n",
    "combined_data['Age'] = combined_data.apply(impute_age, axis=1)\n",
    "\n",
    "# Vérifier que toutes les valeurs d'âge ont été imputées\n",
    "print(f\"\\nValeurs d'âge manquantes après imputation: {combined_data['Age'].isnull().sum()}\")\n",
    "\n",
    "# Si certains titres rares n'ont pas de médiane d'âge dans certaines classes, utiliser la médiane globale par titre\n",
    "if combined_data['Age'].isnull().sum() > 0:\n",
    "    age_by_title = combined_data.groupby('Title')['Age'].transform('median')\n",
    "    combined_data.loc[combined_data['Age'].isnull(), 'Age'] = age_by_title[combined_data['Age'].isnull()]\n",
    "\n",
    "print(f\"Valeurs d'âge manquantes après la seconde imputation: {combined_data['Age'].isnull().sum()}\")\n",
    "\n",
    "# Imputer toute valeur restante avec la médiane globale\n",
    "if combined_data['Age'].isnull().sum() > 0:\n",
    "    combined_data['Age'].fillna(combined_data['Age'].median(), inplace=True)\n",
    "\n",
    "print(f\"Valeurs d'âge manquantes après imputation finale: {combined_data['Age'].isnull().sum()}\")\n",
    "\n",
    "# Visualiser la distribution de l'âge avant et après imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Distribution originale (utiliser les données d'entraînement d'origine)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_data['Age'].dropna(), kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution originale de l\\'âge (sans valeurs manquantes)')\n",
    "plt.xlabel('Âge')\n",
    "plt.ylabel('Fréquence')\n",
    "\n",
    "# Distribution après imputation\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(combined_data.loc[combined_data['is_train'] == 1, 'Age'], kde=True, bins=30, color='green')\n",
    "plt.title('Distribution de l\\'âge après imputation')\n",
    "plt.xlabel('Âge')\n",
    "plt.ylabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a364a",
   "metadata": {},
   "source": [
    "### 3.2 Imputation des autres valeurs manquantes\n",
    "\n",
    "Gérons maintenant les valeurs manquantes pour le port d'embarquement et le tarif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705570d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation du port d'embarquement (Embarked)\n",
    "# Vérifier les valeurs manquantes\n",
    "print(f\"Valeurs manquantes pour 'Embarked': {combined_data['Embarked'].isnull().sum()}\")\n",
    "\n",
    "if combined_data['Embarked'].isnull().sum() > 0:\n",
    "    # Afficher les lignes avec des valeurs manquantes\n",
    "    print(\"\\nPassagers avec port d'embarquement manquant:\")\n",
    "    display(combined_data[combined_data['Embarked'].isnull()])\n",
    "    \n",
    "    # Imputer avec le mode (valeur la plus fréquente)\n",
    "    most_common_port = combined_data['Embarked'].mode()[0]\n",
    "    combined_data['Embarked'].fillna(most_common_port, inplace=True)\n",
    "    print(f\"\\nPort d'embarquement '{most_common_port}' utilisé pour imputer les valeurs manquantes.\")\n",
    "\n",
    "# Imputation du tarif (Fare)\n",
    "print(f\"\\nValeurs manquantes pour 'Fare': {combined_data['Fare'].isnull().sum()}\")\n",
    "\n",
    "if combined_data['Fare'].isnull().sum() > 0:\n",
    "    # Afficher les lignes avec des valeurs manquantes\n",
    "    print(\"\\nPassagers avec tarif manquant:\")\n",
    "    display(combined_data[combined_data['Fare'].isnull()])\n",
    "    \n",
    "    # Imputer avec la médiane par classe\n",
    "    median_fare_by_class = combined_data.groupby('Pclass')['Fare'].median()\n",
    "    \n",
    "    for pclass, fare in median_fare_by_class.items():\n",
    "        combined_data.loc[(combined_data['Fare'].isnull()) & (combined_data['Pclass'] == pclass), 'Fare'] = fare\n",
    "    \n",
    "    print(\"\\nTarifs médians par classe utilisés pour l'imputation:\")\n",
    "    display(median_fare_by_class)\n",
    "\n",
    "# Vérifier que toutes les valeurs manquantes ont été imputées\n",
    "missing_after = combined_data.isnull().sum()\n",
    "print(\"\\nValeurs manquantes après imputation:\")\n",
    "display(missing_after[missing_after > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680a159",
   "metadata": {},
   "source": [
    "## 4. Création de nouvelles caractéristiques\n",
    "\n",
    "Créons maintenant des caractéristiques qui pourraient être utiles pour la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c764a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Taille de la famille et passagers voyageant seuls\n",
    "combined_data['FamilySize'] = combined_data['SibSp'] + combined_data['Parch'] + 1  # +1 pour inclure le passager\n",
    "combined_data['IsAlone'] = (combined_data['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Créer des catégories de tailles de famille\n",
    "combined_data['FamilyGroup'] = pd.cut(\n",
    "    combined_data['FamilySize'],\n",
    "    bins=[0, 1, 4, 11],\n",
    "    labels=['None', 'Small', 'Large']\n",
    ")\n",
    "\n",
    "# 4.2 Extraction des informations sur la cabine\n",
    "# Identifier si le passager a une cabine enregistrée\n",
    "combined_data['HasCabin'] = combined_data['Cabin'].notna().astype(int)\n",
    "\n",
    "# Extraire le pont de la cabine (première lettre)\n",
    "combined_data['Deck'] = combined_data['Cabin'].str.slice(0, 1)\n",
    "combined_data['Deck'].fillna('U', inplace=True)  # U pour \"Unknown\" (inconnu)\n",
    "\n",
    "# 4.3 Créer des groupes d'âge\n",
    "combined_data['AgeGroup'] = pd.cut(\n",
    "    combined_data['Age'],\n",
    "    bins=[0, 12, 18, 35, 60, 100],\n",
    "    labels=['Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\n",
    ")\n",
    "\n",
    "# 4.4 Créer des groupes de tarifs\n",
    "combined_data['FareGroup'] = pd.qcut(\n",
    "    combined_data['Fare'],\n",
    "    q=5,\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "# 4.5 Créer une caractéristique pour le nom de famille\n",
    "# Extraire le nom de famille\n",
    "combined_data['Surname'] = combined_data['Name'].str.split(',').str[0]\n",
    "\n",
    "# 4.6 Créer une caractéristique pour identifier les femmes mariées\n",
    "combined_data['IsMarriedWoman'] = ((combined_data['Title'] == 'Mrs') | \n",
    "                                 (combined_data['Title'] == 'Mme')).astype(int)\n",
    "\n",
    "# Aperçu des nouvelles caractéristiques\n",
    "print(\"Aperçu des nouvelles caractéristiques:\")\n",
    "columns_to_display = ['PassengerId', 'Title', 'Age', 'AgeGroup', 'FamilySize', \n",
    "                     'FamilyGroup', 'IsAlone', 'HasCabin', 'Deck', 'FareGroup', 'IsMarriedWoman']\n",
    "display(combined_data[columns_to_display].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c6eae",
   "metadata": {},
   "source": [
    "## 5. Encodage des variables catégorielles\n",
    "\n",
    "Encodons les variables catégorielles pour les rendre utilisables par les algorithmes de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186726e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Encodage du sexe (Sex)\n",
    "combined_data['Sex_Code'] = combined_data['Sex'].map({'female': 1, 'male': 0})\n",
    "\n",
    "# 5.2 Encodage du port d'embarquement (Embarked)\n",
    "# One-hot encoding pour Embarked\n",
    "embarked_dummies = pd.get_dummies(combined_data['Embarked'], prefix='Embarked')\n",
    "combined_data = pd.concat([combined_data, embarked_dummies], axis=1)\n",
    "\n",
    "# 5.3 Encodage du titre (Title)\n",
    "# One-hot encoding pour Title\n",
    "title_dummies = pd.get_dummies(combined_data['Title'], prefix='Title')\n",
    "combined_data = pd.concat([combined_data, title_dummies], axis=1)\n",
    "\n",
    "# 5.4 Encodage du pont (Deck)\n",
    "# One-hot encoding pour Deck\n",
    "deck_dummies = pd.get_dummies(combined_data['Deck'], prefix='Deck')\n",
    "combined_data = pd.concat([combined_data, deck_dummies], axis=1)\n",
    "\n",
    "# 5.5 Encodage des groupes d'âge (AgeGroup)\n",
    "# One-hot encoding pour AgeGroup\n",
    "age_group_dummies = pd.get_dummies(combined_data['AgeGroup'], prefix='AgeGroup')\n",
    "combined_data = pd.concat([combined_data, age_group_dummies], axis=1)\n",
    "\n",
    "# 5.6 Encodage des groupes de taille de famille (FamilyGroup)\n",
    "# One-hot encoding pour FamilyGroup\n",
    "family_group_dummies = pd.get_dummies(combined_data['FamilyGroup'], prefix='FamilyGroup')\n",
    "combined_data = pd.concat([combined_data, family_group_dummies], axis=1)\n",
    "\n",
    "# 5.7 Encodage des groupes de tarifs (FareGroup)\n",
    "# One-hot encoding pour FareGroup\n",
    "fare_group_dummies = pd.get_dummies(combined_data['FareGroup'], prefix='FareGroup')\n",
    "combined_data = pd.concat([combined_data, fare_group_dummies], axis=1)\n",
    "\n",
    "# Afficher les nouvelles dimensions du DataFrame\n",
    "print(f\"Dimensions du DataFrame après encodage: {combined_data.shape[0]} lignes et {combined_data.shape[1]} colonnes\")\n",
    "print(f\"Nouvelles colonnes créées: {combined_data.shape[1] - 18}\")  # 18 colonnes d'origine\n",
    "\n",
    "# Aperçu des premières colonnes du DataFrame\n",
    "display(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccb14e",
   "metadata": {},
   "source": [
    "## 6. Sélection et préparation des caractéristiques finales\n",
    "\n",
    "Sélectionnons maintenant les caractéristiques pertinentes pour l'entraînement des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241597c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Définir les caractéristiques à conserver pour le modèle\n",
    "features_to_use = [\n",
    "    # Caractéristiques numériques\n",
    "    'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize',\n",
    "    \n",
    "    # Encodage binaire\n",
    "    'Sex_Code', 'IsAlone', 'HasCabin', 'IsMarriedWoman',\n",
    "    \n",
    "    # Variables encodées (one-hot)\n",
    "    'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
    "    'Title_Mr', 'Title_Mrs', 'Title_Miss', 'Title_Master', 'Title_Rare',\n",
    "    \n",
    "    # Ajout des caractéristiques de pont (sélectionner les plus pertinentes)\n",
    "    'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_U',\n",
    "    \n",
    "    # Groupes d'âge\n",
    "    'AgeGroup_Child', 'AgeGroup_Teenager', 'AgeGroup_Young Adult', 'AgeGroup_Adult', 'AgeGroup_Senior',\n",
    "    \n",
    "    # Groupes de famille\n",
    "    'FamilyGroup_None', 'FamilyGroup_Small', 'FamilyGroup_Large'\n",
    "    \n",
    "    # Nous excluons les groupes de tarif car ils pourraient être redondants avec 'Fare'\n",
    "]\n",
    "\n",
    "# Afficher le nombre de caractéristiques sélectionnées\n",
    "print(f\"Nombre de caractéristiques sélectionnées: {len(features_to_use)}\")\n",
    "\n",
    "# 6.2 Création des ensembles d'entraînement et de test finaux\n",
    "# Séparer les ensembles d'entraînement et de test\n",
    "train_processed = combined_data.loc[combined_data['is_train'] == 1, features_to_use].reset_index(drop=True)\n",
    "test_processed = combined_data.loc[combined_data['is_train'] == 0, features_to_use].reset_index(drop=True)\n",
    "\n",
    "print(f\"Ensemble d'entraînement prétraité: {train_processed.shape[0]} lignes et {train_processed.shape[1]} colonnes\")\n",
    "print(f\"Ensemble de test prétraité: {test_processed.shape[0]} lignes et {test_processed.shape[1]} colonnes\")\n",
    "\n",
    "# Aperçu des données prétraitées\n",
    "print(\"\\nAperçu des données d'entraînement prétraitées:\")\n",
    "display(train_processed.head())\n",
    "\n",
    "# 6.3 Normalisation des caractéristiques numériques\n",
    "numerical_features = ['Age', 'Fare', 'FamilySize', 'SibSp', 'Parch']\n",
    "\n",
    "# Créer un scaler pour les caractéristiques numériques\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajuster le scaler sur les données d'entraînement et transformer les deux ensembles\n",
    "train_processed[numerical_features] = scaler.fit_transform(train_processed[numerical_features])\n",
    "test_processed[numerical_features] = scaler.transform(test_processed[numerical_features])\n",
    "\n",
    "# Vérifier les statistiques après normalisation\n",
    "print(\"\\nStatistiques des caractéristiques numériques après normalisation (ensemble d'entraînement):\")\n",
    "display(train_processed[numerical_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73cb08",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde des données prétraitées\n",
    "\n",
    "Sauvegardons les ensembles de données prétraités pour les utiliser lors de la phase de modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire avec les objets importants\n",
    "preprocessed_data = {\n",
    "    'train_features': train_processed,\n",
    "    'train_target': train_targets,\n",
    "    'test_features': test_processed,\n",
    "    'test_passenger_ids': test_passenger_ids,\n",
    "    'feature_names': features_to_use,\n",
    "    'scaler': scaler  # Pour appliquer la même normalisation aux données futures\n",
    "}\n",
    "\n",
    "# Chemin de sauvegarde\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Créer le dossier pour les données prétraitées s'il n'existe pas\n",
    "preprocessed_dir = '/workspaces/titanicML/Data/preprocessed'\n",
    "if not os.path.exists(preprocessed_dir):\n",
    "    os.makedirs(preprocessed_dir)\n",
    "\n",
    "# Sauvegarder les données prétraitées\n",
    "joblib.dump(preprocessed_data, os.path.join(preprocessed_dir, 'preprocessed_data.pkl'))\n",
    "\n",
    "print(f\"Données prétraitées sauvegardées dans {preprocessed_dir}\")\n",
    "\n",
    "# Sauvegarder également au format CSV pour une inspection facile\n",
    "train_processed_with_target = train_processed.copy()\n",
    "train_processed_with_target['Survived'] = train_targets\n",
    "\n",
    "train_processed_with_target.to_csv(os.path.join(preprocessed_dir, 'train_processed.csv'), index=False)\n",
    "test_processed.to_csv(os.path.join(preprocessed_dir, 'test_processed.csv'), index=False)\n",
    "\n",
    "print(\"Fichiers CSV également générés pour une inspection facile des données prétraitées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67522d",
   "metadata": {},
   "source": [
    "## 8. Conclusions et prochaines étapes\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "\n",
    "1. **Géré les valeurs manquantes**:\n",
    "   - Imputé les âges manquants en fonction du titre et de la classe\n",
    "   - Imputé le port d'embarquement manquant avec la valeur la plus fréquente\n",
    "   - Imputé les tarifs manquants avec la médiane par classe\n",
    "\n",
    "2. **Créé de nouvelles caractéristiques**:\n",
    "   - Taille de la famille et indicateur pour les passagers voyageant seuls\n",
    "   - Extraction des titres à partir des noms\n",
    "   - Information sur la présence d'une cabine et le pont\n",
    "   - Groupes d'âge et de tarifs\n",
    "\n",
    "3. **Encodé les variables catégorielles**:\n",
    "   - Encodage binaire pour le sexe\n",
    "   - One-hot encoding pour les autres variables catégorielles\n",
    "\n",
    "4. **Normalisé les caractéristiques numériques**:\n",
    "   - Application du StandardScaler pour mettre les variables à la même échelle\n",
    "\n",
    "Les données sont maintenant prêtes pour la phase de modélisation, où nous testerons différents algorithmes de classification pour prédire la survie des passagers du Titanic.\n",
    "\n",
    "### Prochaines étapes:\n",
    "- Entraîner différents modèles de classification (Random Forest, XGBoost, etc.)\n",
    "- Optimiser les hyperparamètres des modèles\n",
    "- Évaluer les performances des modèles\n",
    "- Sélectionner le meilleur modèle pour la prédiction finale"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
